{"cells":[{"cell_type":"markdown","metadata":{"id":"cabwTdKAW7x_"},"source":["<br>\n","\n","\n","# Introdução ao Aprendizado por Reforço (Reinforcement Learning)\n","\n","## Algoritmo Q-learning\n","\n","O algoritmo __Q-Learning__ é um dos mais fundamentais e conhecidos algoritmos nesse contexto de aprendizado por reforço. \n","Iremos estudar no detalhe esse algoritmo e implementá-lo do zero. Iremos também nos basear num _problema prático_, o que aumentará o entendimento no tema!\n","\n","<br>\n","\n","__Exemplo prático:__\n","O exemplo que trabalharemos descreve um agente que usa aprendizado não supervisionado (Q-learning) para aprender a respeito de um ambiente desconhecido.\n","\n","\n","Suponha que estamos numa casa com 5 cômodos conectados por portas, como mostrado na figura abaixo. Vamos enumerar cada cômodo de 0 até 4. O \"cômodo\" 5 representa _o lado de fora da casa_.\n","\n","Nosso agente começará num cômodo qualquer da casa e o objetivo é que ele aprenda a sair da casa da forma mais eficiente possível, ou seja, passando pela menor quantidade de cômodos!\n","\n","\n","Planta da casa com a indicação dos cômodos:"]},{"cell_type":"markdown","metadata":{"id":"kUfpbn5BW7yB"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"2TvJtZqpW7yC"},"source":["<br>\n","\n","Podemos representar a planta da casa como um __grafo__, em que cada nó (ou estado - state) representa um cômodo e as arestas representam as possíveis movimentações dentro da casa.\n","\n","Por exemplo, do cômodo 2, podemos ir apenas para o cômodo 3. Já no cômodo 4, podemos ir para 0, 3 ou 5.\n","\n","O nó (state) 5 representa nosso __goal state__, ou seja, é o nosso objetivo!\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"sD3b39zbW7yC"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"JPJBgyScW7yD"},"source":["<br>\n","\n","Como já foi comentado, o objetivo é colocar um agente num cômodo qualquer e que esse agente seja capaz de conseguir sair de casa de forma otimizada.\n","\n","Como o agente, no início, não conhece o ambiente, ele irá explorar: irá percorrer de forma aleatória o ambiente para aprender. \n","\n","Iremos __premiar o agente de acordo com quais ações ele tomar.__ Por exemplo, se o agente está no cômodo 1 e ele decide ir para o cômodo 5, iremos dar uma recompensa positiva, pois ele fez um movimento correto. Caso ele opte em ir do cômodo 1 para o 3, a recompensa não será positiva, pois a ação não foi a melhor. \n","\n","Para isso, precisamos __definir quais são as recompensas.__ Faremos isso de forma numérica, veja a figura abaixo:\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"LcYXlKm5W7yD"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"zcXL6qY0W7yD"},"source":["<br>\n","\n","Aqui, a recompensa é clara: se a ação tomada nos encaminhar para o nosso objetivo (goal state = cômodo 5), então atribuimos um valor 100. Caso contrário, atribuimos um valor 0. \n","\n","Ainda, o cômodo 5 tem um loop nele mesmo com recompensa de 100 pontos, indicando que, ao chegarmos no cômodo 5, queremos permanecer neste cômodo!\n","\n","__No Q-Learning, o objetivo do algoritmo é aprender a se deslocar para estados (states) com as maiores recompensas.__\n","\n","<br><br>\n","\n","\n","Agora, de forma prática, imagine que o nosso agente está no cômodo 2 e é solicitado para evacuar a casa, ou seja, o agente precisa ir para o cômodo 5:\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"2IJQ-tg0W7yE"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"dqIeUATmW7yE"},"source":["Como a __terminologia__ do __Q-Learning__ envolve os termos __estado__ e __ação__, vamos utilizá-los sempre a partir de agora.\n","\n","Reforçando, cada cômodo da casa é um estado; o estado 5 é o estado objetivo, que significa sair da casa.\n","\n","Além disso, o movimento do agente de ir de um cômodo para outro é o que chamamos de ação.\n","\n","Por exemplo, na prática, o agente tomar a ação de ir do estado 1 para o estado 3 significa que o agente se movimentou do cômodo 1 para o cômodo 3. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"GSwD-Bo5W7yF"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"sowZEiuxW7yF"},"source":["<br>\n","\n","Acima, temos o agente começando no estado 2. Neste caso, a única ação que ele pode tomar é ir para o estado 3.\n","Chegando no estado 3, ele pode tomar várias ações: voltar para 2, ir para 1 ou para 4. Caso ele vá para 1 ou 4, ele pode ter a chance de chegar no estado 5. \n","\n","<br><br>\n","\n","Podemos __representar o grafo de estados/ações/recompensas__ de uma __forma matricial!__ Essa representação será fundamental para nossa implementação do Q-Learning.\n","\n","Abaixo, temos a foto da matriz __R__: a matriz de recompensas ou reward matrix.\n","\n","<br>\n","\n","### Reward matrix:"]},{"cell_type":"markdown","metadata":{"id":"M1omMd6DW7yG"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"Ir94I16BW7yG"},"source":["<br>\n","\n","Cada entrada da matriz registra o valor da recompensa em ir de um estado para outro (ou seja, qual a recompensa atrelada a ação). Além disso, o valor -1 indica que uma ação não é possível de ser tomada. \n","\n","Por exemplo, não é possível ir do estado 0 para o estado 2, então representamos essa impossibilidade colocando -1 na entrada atrelada a esse par na matriz."]},{"cell_type":"markdown","metadata":{"id":"u3UMPB3UW7yH"},"source":["<br>\n","\n","Além da matriz de recompensas __R__ apresentada acima, outra matriz muito importante nesse contexto é a __matriz Q__.\n","\n","De fato, essa matriz pode ser interpretada como o __cérebro do agente__, representando a memória do que o agente aprendeu do ambiente desde que ele começou a exploração. E claro, o __Q__ do Q-Learning é justamente fazendo menção a essa matriz! :)\n","\n","<br>\n","\n","\n","__O Algoritmo Q-learning funciona com a seguinte lógica:__\n","\n","Como o agente começa o aprendizado sem nenhum conhecimento, a matriz será inicializada totalmente zerada. \n","\n","Iremos representar a matriz Q com os mesmos formatos da matriz R, dado que sabemos - neste caso - quais são os estados possíveis para o agente se movimentar. \n","\n","A medida que o agente vai andando pelo ambiente, isto é, tomando ações, vamos __atualizando a matriz Q__, representando o aprendizado do agente a respeito do ambiente.\n","\n","__A regra de atualização da matriz é expressa pela fórmula:__\n","\n","Suponha que o agente está no estado S e toma a ação A, indo para o estado $S_{post}$ (estado posterior).\n","\n","representação:\n","##  S  $\\xrightarrow{\\text{ação A}}$  $S_{post}$  \n","\n","<br>\n","\n","iremos atualizar a matriz Q com a regra:\n","\n","#### Q(S, A) = R(S, A) + $\\gamma$ * Max[ Q($S_{post}$, todas ações possíveis) ]\n","\n","<br>\n","\n","De acordo com a fórmula acima, a atualização da entrada (S,A) da matriz Q será pela soma da recompensa atrelada a tomar a ação A a partir do estado S (que podemos descobrir consultando diretamente a matriz R) com a multiplicação entre $\\gamma$ (um fator de correção, a ser explicado na sequência) e o valor máximo de Q para todas as ações possíveis no próximo estado, $S_{post}$.\n","\n","<br>\n","\n","O agente irá aprender pela experiência das tomadas de ações e fará isso de forma não supervisionada. \n","\n","Ele irá explorar o ambiente a partir de um primeiro estado até atingir o estado objetivo. Quando ele atingir o objetivo, teremos concluído um __episódio__ de aprendizado. \n","\n","Em tempos de treinamento, o agente sempre começa um estado aleatório e explora o ambiente até atingir o estado objetivo, isto é, até completar um episódio. Quando um episódio é completado, outro é iniciado de forma semelhante. \n","\n","A medida que mais episódios são completados, mais o agente aprende sobre o ambiente, visto que a matriz Q vai \"evoluindo\" a medida que os episódios são finalizados.\n","\n","<br><br>\n","\n","__O algoritmo pode ser expresso como segue:__\n","\n","1) Defina o parâmetro $\\gamma$ e a matriz de recompensa R a partir dos estados e ações possíveis a ser tomadas;\n","\n","2) Inicialize a matriz Q com zeros;\n","\n","3) (Loop nos episódios) Para cada episódio:\n","\n","    3.1) selecione um estado inicial de forma aleatória;\n","    \n","    3.2) enquanto o estado objetivo não é alcançado:\n","    \n","            3.2.1) selecione uma dentre todas as possíveis ações a partir do estado selecionado\n","            \n","            3.2.2) com a ação selecionada (o que já indica qual é o próximo estado), calcule\n","            \n","Q(S, A) = R(S, A) + $\\gamma$ * Max[ Q($S_{post}$, todas ações possíveis) ]\n","                   \n","            3.2.3) defina o novo estado inicial como sendo o estado resultante da ação tomada\n","            \n","    3.3) Finalize o episódio quando o agente atingir o estado objetivo. \n","\n","4) A matriz Q, atualizada após o loop nos episódios, é utilizada para o agente tomar as decisões.\n","\n","<br><br>\n","\n","__Comentários finais:__\n","\n","O algoritmo acima (Q-Learning) é usado para o agente aprender pela experiência de exploração. Cada episódio finalizado é equivalente a uma seção de treinamento. Nessas seções, o agente explora o ambiente e recebe recompensas (positivas ou não) de acordo com as ações tomadas, até atingir o estado objetivo. \n","\n","O objetivo do treinamento é refinar o 'cérebro' do agente, isto é, refinar a matriz Q. Uma vez que o agente seja bastante treinado - ou seja, tendo completado muitos episódios - em vez do agente ficar \"rodando\" pela casa, indo de comôdo em cômodo de forma aleatória, ele vai conseguir achar o caminho da saída da forma mais otimizada!\n","\n","Além disso, o __parâmetro $\\gamma$__, varia entre 0 e 1. Se $\\gamma$ for próximo de 0, o agente irá tender a tomar ações baseadas em resultados imediatos. Caso $\\gamma$ seja próximo de 1, o agente tende a considerar recompensas futuras com mais peso. É um hiper-parâmetro do algoritmo e, como todas as escolhas em Machine Learning, precisa ser muito bem testada de acordo com o problema em questão.\n","\n","Finalmente, __uma vez que o treinamento tenha sido concluído,__ o agente tomará as decisões diretamente da matriz Q.\n","\n","Partindo de um estado inicial, o agente irá tomar a ação com o maior valor de Q e irá para o próximo estado. Ele fará isso repetidamente até atingir o estado objetivo, no qual permancerá.\n","\n","\n","<br><br>\n","\n","Vamos abaixo começar a discutir e exemplificar esses pontos todos em Python!\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvRBWaGNW7yJ"},"outputs":[],"source":["#importando os módulos básicos\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kX6EjkqPW7yL"},"outputs":[],"source":["#definindo quais são os estados e ações que podemos tomar\n","states = [0,1,2,3,4,5]\n","actions = [0,1,2,3,4,5]\n","\n","#definido o parâmetro gamma e o estado objetivo\n","gamma = 0.8\n","goal_state = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03eo3RZ5W7yL","outputId":"84096304-7734-4372-e940-79313385a3ae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306104828,"user_tz":180,"elapsed":25,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["reward matrix: matriz de recompensas\n","\n","         state_0  state_1  state_2  state_3  state_4  state_5\n","state_0       -1       -1       -1       -1        0       -1\n","state_1       -1       -1       -1        0       -1      100\n","state_2       -1       -1       -1        0       -1       -1\n","state_3       -1        0        0       -1        0       -1\n","state_4        0       -1       -1        0       -1      100\n","state_5       -1        0       -1       -1        0      100\n"]}],"source":["#definindo a matriz de recompensa\n","R = [[-1,-1,-1,-1,0,-1], \n","     [-1,-1,-1,0,-1,100],\n","     [-1,-1,-1,0,-1,-1],\n","     [-1,0,0,-1,0,-1],\n","     [0,-1,-1,0,-1,100],\n","     [-1,0,-1,-1,0,100]]\n","\n","R = np.array(R)\n","print(\"reward matrix: matriz de recompensas\\n\")\n","print(pd.DataFrame(R,index=['state_'+str(i) for i in range(R.shape[0])],\n","                     columns=['state_'+str(i) for i in range(R.shape[0])],))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YIS1RO5W7yM","outputId":"178cae4b-0c10-4f69-aace-76162d5e115b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306109889,"user_tz":180,"elapsed":18,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Q-matrix: cérebro do agente, inicializado sem nenhum conhecimento do ambiente\n","\n","         state_0  state_1  state_2  state_3  state_4  state_5\n","state_0      0.0      0.0      0.0      0.0      0.0      0.0\n","state_1      0.0      0.0      0.0      0.0      0.0      0.0\n","state_2      0.0      0.0      0.0      0.0      0.0      0.0\n","state_3      0.0      0.0      0.0      0.0      0.0      0.0\n","state_4      0.0      0.0      0.0      0.0      0.0      0.0\n","state_5      0.0      0.0      0.0      0.0      0.0      0.0\n"]}],"source":["#inicializando a matriz Q - totalmente zerada\n","Q = np.zeros(len(states) * len(actions)).reshape(len(states), len(actions))\n","print(\"Q-matrix: cérebro do agente, inicializado sem nenhum conhecimento do ambiente\\n\")\n","print(pd.DataFrame(Q,index=['state_'+str(i) for i in range(R.shape[0])],\n","                     columns=['state_'+str(i) for i in range(R.shape[0])],))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_-ZMATeW7yN"},"outputs":[],"source":["#função auxiliar: a partir da matriz R, a função retorna quais são os possíveis estado a partir de um estado fixado\n","def get_possible_next_states(R, state):\n","    possible_next_states = np.argwhere(R[state, :]>=0).reshape(-1,)\n","    return possible_next_states"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FD9Ba69yW7yN","outputId":"aed4e64a-d1db-451d-8252-671abc459391","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306112966,"user_tz":180,"elapsed":13,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4])"]},"metadata":{},"execution_count":6}],"source":["#exemplos\n","get_possible_next_states(R, state = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIPYrASrW7yO","outputId":"262c959c-421b-4cd0-f3fe-4e3f518481e9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306113463,"user_tz":180,"elapsed":18,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 4])"]},"metadata":{},"execution_count":7}],"source":["#exemplos\n","get_possible_next_states(R, state = 3)"]},{"cell_type":"markdown","metadata":{"id":"nLN2KCivW7yO"},"source":["<br><br>"]},{"cell_type":"markdown","metadata":{"id":"VDwUtskVW7yO"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"5TPuRKEsW7yP"},"source":["<br><br>"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"YU9kgSU8W7yQ","outputId":"7767958b-18fe-4245-b718-6bf12b05edb8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306115519,"user_tz":180,"elapsed":25,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["state --> next state: 0 4\n","\n","state --> next state: 4 3\n","\n","state --> next state: 3 2\n","\n","state --> next state: 2 3\n","\n","state --> next state: 3 2\n","\n","state --> next state: 2 3\n","\n","state --> next state: 3 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 3\n","\n","state --> next state: 3 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 0\n","\n","state --> next state: 0 4\n","\n","state --> next state: 4 0\n","\n","state --> next state: 0 4\n","\n","state --> next state: 4 5\n","\n","fim do episódio\n","\n","Q-matrix atualizada:\n","[[  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0. 100.]\n"," [  0.   0.   0.   0.   0.   0.]]\n"]}],"source":["#### código para construir um episódio ####\n","\n","#começamos escolhendo um estado de forma aleatórioa\n","state = np.random.choice(states, size = 1)[0]\n","\n","#loop while até atingirmos o estado objetivo\n","next_state = None\n","while next_state != goal_state:\n","    \n","    #selecionamos os próximos estados, a partir do estado inicial\n","    possible_next_states = get_possible_next_states(R, state)\n","    \n","    #definimos o próximo estado, ou seja, estamos caracterizando qual ação foi tomada\n","    next_state = np.random.choice(possible_next_states, size = 1)[0]\n","    print('state --> next state:', state, next_state)\n","    \n","    #em cima da ação tomada, atualizamos a matriz Q\n","    M = Q[next_state, get_possible_next_states(R, next_state)].max()\n","    Q[state, next_state] = R[state, next_state] + gamma * M\n","\n","    #atribuimos o estado inicial ao novo estado\n","    state = next_state\n","    print()\n","    \n","print('fim do episódio')\n","print()\n","print('Q-matrix atualizada:')\n","print(Q)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gI6oTeqvW7yQ","outputId":"4ff383b8-476f-41a4-dc5c-07c2a4935e38","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306120181,"user_tz":180,"elapsed":654,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["state --> next state: 5 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 5\n","\n","fim do episódio\n","\n","Q-matrix atualizada:\n","[[  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.  80.   0.]\n"," [  0.   0.   0.   0.   0. 100.]\n"," [  0.   0.   0.   0.   0.   0.]]\n"]}],"source":["#vamos rodar um segundo episódio, para avaliarmos a evolução da matriz Q\n","\n","state = np.random.choice(states, size = 1)[0]\n","\n","next_state = None\n","while next_state != goal_state:\n","    \n","    possible_next_states = get_possible_next_states(R, state)\n","    next_state = np.random.choice(possible_next_states, size = 1)[0]\n","    print('state --> next state:', state, next_state)\n","    M = Q[next_state, get_possible_next_states(R, next_state)].max()\n","    Q[state, next_state] = R[state, next_state] + gamma * M\n","\n","    state = next_state\n","    print()\n","    \n","print('fim do episódio')\n","print()\n","print('Q-matrix atualizada:')\n","print(Q)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnisKC92W7yR","outputId":"c3865a00-59de-4fb8-90ef-509ec9538c94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306122595,"user_tz":180,"elapsed":20,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["state --> next state: 5 5\n","\n","fim do episódio\n","\n","Q-matrix atualizada:\n","[[  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.  80.   0.]\n"," [  0.   0.   0.   0.   0. 100.]\n"," [  0.   0.   0.   0.   0. 100.]]\n"]}],"source":["#terceiro episódio\n","\n","state = np.random.choice(states, size = 1)[0]\n","\n","next_state = None\n","while next_state != goal_state:\n","    \n","    possible_next_states = get_possible_next_states(R, state)\n","    next_state = np.random.choice(possible_next_states, size = 1)[0]\n","    print('state --> next state:', state, next_state)\n","    M = Q[next_state, get_possible_next_states(R, next_state)].max()\n","    Q[state, next_state] = R[state, next_state] + gamma * M\n","\n","    state = next_state\n","    print()\n","    \n","print('fim do episódio')\n","print()\n","print('Q-matrix atualizada:')\n","print(Q)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuuGphgcW7yR","outputId":"b295147e-293a-4534-dc58-50efbbd0edad","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306125193,"user_tz":180,"elapsed":1314,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["state --> next state: 5 1\n","\n","state --> next state: 1 5\n","\n","fim do episódio\n","\n","Q-matrix atualizada:\n","[[  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0. 180.]\n"," [  0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.  80.   0.]\n"," [  0.   0.   0.   0.   0. 100.]\n"," [  0.   0.   0.   0.   0. 100.]]\n"]}],"source":["#quarto episódio...\n","\n","state = np.random.choice(states, size = 1)[0]\n","\n","next_state = None\n","while next_state != goal_state:\n","    \n","    possible_next_states = get_possible_next_states(R, state)\n","    next_state = np.random.choice(possible_next_states, size = 1)[0]\n","    print('state --> next state:', state, next_state)\n","    M = Q[next_state, get_possible_next_states(R, next_state)].max()\n","    Q[state, next_state] = R[state, next_state] + gamma * M\n","\n","    state = next_state\n","    print()\n","    \n","print('fim do episódio')\n","print()\n","print('Q-matrix atualizada:')\n","print(Q)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JULvcMMHW7yU","outputId":"675854d4-ae52-4dc1-a4d2-54e25d923e23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664306125675,"user_tz":180,"elapsed":13,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["state --> next state: 1 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 3\n","\n","state --> next state: 3 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 0\n","\n","state --> next state: 0 4\n","\n","state --> next state: 4 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 0\n","\n","state --> next state: 0 4\n","\n","state --> next state: 4 0\n","\n","state --> next state: 0 4\n","\n","state --> next state: 4 3\n","\n","state --> next state: 3 1\n","\n","state --> next state: 1 3\n","\n","state --> next state: 3 4\n","\n","state --> next state: 4 0\n","\n","state --> next state: 0 4\n","\n","state --> next state: 4 5\n","\n","fim do episódio\n","\n","Q-matrix atualizada:\n","[[  0.      0.      0.      0.     92.16    0.   ]\n"," [  0.      0.      0.    115.2     0.    180.   ]\n"," [  0.      0.      0.      0.      0.      0.   ]\n"," [  0.    144.      0.      0.     92.16    0.   ]\n"," [ 73.728   0.      0.    115.2     0.    180.   ]\n"," [  0.      0.      0.      0.      0.    100.   ]]\n"]}],"source":["#quinto episódio....\n","\n","state = np.random.choice(states, size = 1)[0]\n","\n","next_state = None\n","while next_state != goal_state:\n","    \n","    possible_next_states = get_possible_next_states(R, state)\n","    next_state = np.random.choice(possible_next_states, size = 1)[0]\n","    print('state --> next state:', state, next_state)\n","    M = Q[next_state, get_possible_next_states(R, next_state)].max()\n","    Q[state, next_state] = R[state, next_state] + gamma * M\n","\n","    state = next_state\n","    print()\n","    \n","print('fim do episódio')\n","print()\n","print('Q-matrix atualizada:')\n","print(Q)"]},{"cell_type":"markdown","metadata":{"id":"-EToko5IW7yV"},"source":["<br><br>\n","\n","Bacana! Fica evidente a evolução da matriz Q ao longo dos episódios.\n","\n","Vamos agora encapsular o algoritmo numa classe para podermos utilizá-lo de forma mais geral!\n","\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0XsbV-aW7yV"},"outputs":[],"source":["class Q_Learning():\n","    \n","    def __init__(self, states, actions, R, goal_state, gamma):\n","        self.states = states\n","        self.actions = actions\n","        self.R = R\n","        self.goal_state = goal_state\n","        self.gamma = gamma\n","        Q = np.zeros(len(states) * len(actions)).reshape(len(states), len(actions))\n","        self.Q = Q\n","    \n","    def get_Qmatrix(self):\n","        Qdf = pd.DataFrame(self.Q, \n","                           index=['state_'+str(i) for i in range(self.Q.shape[0])],\n","                           columns=['action_'+str(i) for i in range(self.Q.shape[1])]).astype(int)\n","        return Qdf\n","        \n","    def run_episode(self):\n","        state = np.random.choice(self.states, size = 1)[0]\n","        next_state = None\n","        while next_state != self.goal_state:\n","            possible_next_states = get_possible_next_states(R, state)\n","            next_state = np.random.choice(possible_next_states, size = 1)[0]\n","            M = self.Q[next_state, get_possible_next_states(R, next_state)].max()\n","            self.Q[state, next_state] = self.R[state, next_state] + self.gamma * M\n","            state = next_state\n","        \n","    def train_agent(self, num_episodes):\n","        for e in range(num_episodes):\n","            self.run_episode()\n","            \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7WqJi87W7yV"},"outputs":[],"source":["#instanciando o agente\n","agente = Q_Learning(states = states, \n","                actions = actions, \n","                R = R, \n","                goal_state = goal_state, \n","                gamma = gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6twGw_xW7yW","outputId":"7fab61e3-91c8-4dd3-ee36-cc834a2a2d4f","colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"status":"ok","timestamp":1664306133827,"user_tz":180,"elapsed":51,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         action_0  action_1  action_2  action_3  action_4  action_5\n","state_0         0         0         0         0         0         0\n","state_1         0         0         0         0         0         0\n","state_2         0         0         0         0         0         0\n","state_3         0         0         0         0         0         0\n","state_4         0         0         0         0         0         0\n","state_5         0         0         0         0         0         0"],"text/html":["\n","  <div id=\"df-344c7144-74f4-4818-8983-26b71a600845\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>action_0</th>\n","      <th>action_1</th>\n","      <th>action_2</th>\n","      <th>action_3</th>\n","      <th>action_4</th>\n","      <th>action_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>state_0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-344c7144-74f4-4818-8983-26b71a600845')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-344c7144-74f4-4818-8983-26b71a600845 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-344c7144-74f4-4818-8983-26b71a600845');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}],"source":["#matriz Q - zerada no início - agente sem conhecimento do ambiente\n","agente.get_Qmatrix()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sj7oW-Y4W7yW"},"outputs":[],"source":["#treinando o agente em apenas 1 episódio\n","agente.train_agent(num_episodes = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIc9rSfFW7yd","outputId":"e718921c-3435-46b0-e258-c65cee67743d","colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"status":"ok","timestamp":1664306136346,"user_tz":180,"elapsed":20,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         action_0  action_1  action_2  action_3  action_4  action_5\n","state_0         0         0         0         0         0         0\n","state_1         0         0         0         0         0       100\n","state_2         0         0         0         0         0         0\n","state_3         0         0         0         0         0         0\n","state_4         0         0         0         0         0         0\n","state_5         0         0         0         0         0         0"],"text/html":["\n","  <div id=\"df-593b7b38-0491-42bf-920a-b76f97d6187f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>action_0</th>\n","      <th>action_1</th>\n","      <th>action_2</th>\n","      <th>action_3</th>\n","      <th>action_4</th>\n","      <th>action_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>state_0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>state_2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-593b7b38-0491-42bf-920a-b76f97d6187f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-593b7b38-0491-42bf-920a-b76f97d6187f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-593b7b38-0491-42bf-920a-b76f97d6187f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}],"source":["agente.get_Qmatrix()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6dk3DTYW7ye","outputId":"ec777bad-09b4-4e7d-a330-a251293e41ee","colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"status":"ok","timestamp":1664306138421,"user_tz":180,"elapsed":29,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         action_0  action_1  action_2  action_3  action_4  action_5\n","state_0         0         0         0         0       195         0\n","state_1         0         0         0       156         0       244\n","state_2         0         0         0       156         0         0\n","state_3         0       195       124         0       195         0\n","state_4       115         0         0       156         0       244\n","state_5         0         0         0         0         0       180"],"text/html":["\n","  <div id=\"df-870f23bd-e4df-4c7b-ac09-be849615468b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>action_0</th>\n","      <th>action_1</th>\n","      <th>action_2</th>\n","      <th>action_3</th>\n","      <th>action_4</th>\n","      <th>action_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>state_0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>195</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>156</td>\n","      <td>0</td>\n","      <td>244</td>\n","    </tr>\n","    <tr>\n","      <th>state_2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>156</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_3</th>\n","      <td>0</td>\n","      <td>195</td>\n","      <td>124</td>\n","      <td>0</td>\n","      <td>195</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_4</th>\n","      <td>115</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>156</td>\n","      <td>0</td>\n","      <td>244</td>\n","    </tr>\n","    <tr>\n","      <th>state_5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>180</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-870f23bd-e4df-4c7b-ac09-be849615468b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-870f23bd-e4df-4c7b-ac09-be849615468b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-870f23bd-e4df-4c7b-ac09-be849615468b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}],"source":["#treinando o agente com alguns episódios\n","agente.train_agent(num_episodes = 10)\n","agente.get_Qmatrix()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ8M06-lW7ye","outputId":"f32fa78b-0833-402e-fbf3-7d6ff7436ef6","colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"status":"ok","timestamp":1664306142106,"user_tz":180,"elapsed":1793,"user":{"displayName":"Milena Menezes Adão","userId":"01995504081397239354"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         action_0  action_1  action_2  action_3  action_4  action_5\n","state_0         0         0         0         0       400         0\n","state_1         0         0         0       320         0       500\n","state_2         0         0         0       320         0         0\n","state_3         0       400       256         0       400         0\n","state_4       320         0         0       320         0       500\n","state_5         0       400         0         0       400       500"],"text/html":["\n","  <div id=\"df-5daf118d-1f09-48f7-a1ac-e8689cb82a54\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>action_0</th>\n","      <th>action_1</th>\n","      <th>action_2</th>\n","      <th>action_3</th>\n","      <th>action_4</th>\n","      <th>action_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>state_0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>400</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>320</td>\n","      <td>0</td>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th>state_2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>320</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_3</th>\n","      <td>0</td>\n","      <td>400</td>\n","      <td>256</td>\n","      <td>0</td>\n","      <td>400</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>state_4</th>\n","      <td>320</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>320</td>\n","      <td>0</td>\n","      <td>500</td>\n","    </tr>\n","    <tr>\n","      <th>state_5</th>\n","      <td>0</td>\n","      <td>400</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>400</td>\n","      <td>500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5daf118d-1f09-48f7-a1ac-e8689cb82a54')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5daf118d-1f09-48f7-a1ac-e8689cb82a54 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5daf118d-1f09-48f7-a1ac-e8689cb82a54');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["#treinando o agente por um longo tempo (muitos episódios)\n","agente.train_agent(num_episodes = 5000)\n","agente.get_Qmatrix()"]},{"cell_type":"markdown","metadata":{"id":"k1RLSuh1W7yf"},"source":["<br>\n","\n","É isso!\n","\n","com essa última versão da matriz Q, fica evidente que o agente sabe quais são as melhores ações para se tomar:\n","    \n","    - caso ele não esteja no estado objetivo, ele sabe chegar lá de forma otimizada\n","    \n","    - caso ele já esteja no estado objetivo, ele reconhece que deve permanecer no estado\n","\n","\n","<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n","\n","### Exercício\n","\n","Considere o grafo abaixo:\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"D8xGw3tAW7yf"},"source":["<div>\n","<img src=\"attachment:image.png\" width=\"600\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"T68in8QxW7yf"},"source":["<br>\n","\n","Podemos interpretar esse grafo como sendo um _labirinto_ em que o objetivo é fazer um agente aprender a sair dele no menor número de passos!\n","\n","Utilize os estados/ações definidos abaixo, bem como caracterizando o estado objetivo.\n","\n","Utilize a representação da matriz de recompensas a partir do grafo e aplique o algoritmo Q-Learning para que o agente aprenda a escapar.\n","\n","Comece treinando o agente com poucos episódios e __vá interpretando os resultados__ a medida que a quantidade de episódios aumente. \n","\n","__Crie uma função__ que recebe dois parâmetros: a matriz Q e um estado inicial. Retorne qual é o caminho que o agente sugere, a partir desse estado inicial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kT_a1BY3W7yg"},"outputs":[],"source":["states = [0,1,2,3,4,5,6,7]\n","actions = [0,1,2,3,4,5,6,7]\n","goal_state = 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wbV6U4cW7yg","outputId":"f38f770d-0cf5-420e-ee72-9bed5216a525"},"outputs":[{"name":"stdout","output_type":"stream","text":["reward matrix: matriz de recompensas\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state_0</th>\n","      <th>state_1</th>\n","      <th>state_2</th>\n","      <th>state_3</th>\n","      <th>state_4</th>\n","      <th>state_5</th>\n","      <th>state_6</th>\n","      <th>state_7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>state_0</th>\n","      <td>-1</td>\n","      <td>80</td>\n","      <td>5</td>\n","      <td>100</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>state_1</th>\n","      <td>30</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>80</td>\n","      <td>100</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>state_2</th>\n","      <td>100</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>state_3</th>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>30</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>100</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>state_4</th>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>5</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>100</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>state_5</th>\n","      <td>-1</td>\n","      <td>30</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>80</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>state_6</th>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>30</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>state_7</th>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         state_0  state_1  state_2  state_3  state_4  state_5  state_6  \\\n","state_0       -1       80        5      100       -1       -1       -1   \n","state_1       30       -1       -1       -1       80      100       -1   \n","state_2      100       -1       -1       -1       -1       -1       -1   \n","state_3       -1       -1       30       -1       -1       -1      100   \n","state_4       -1       -1        5       -1       -1      100       -1   \n","state_5       -1       30       -1       -1       -1       -1       80   \n","state_6       -1       -1       -1       -1       30       -1       -1   \n","state_7       -1       -1       -1       -1       -1       -1       -1   \n","\n","         state_7  \n","state_0       -1  \n","state_1       -1  \n","state_2       -1  \n","state_3       -1  \n","state_4       -1  \n","state_5      100  \n","state_6      100  \n","state_7      100  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["R = [[-1,80,5,100,-1,-1,-1,-1],\n","     [30,-1,-1,-1,80,100,-1,-1],\n","     [100,-1,-1,-1,-1,-1,-1,-1],\n","     [-1,-1,30,-1,-1,-1,100,-1],\n","     [-1,-1,5,-1,-1,100,-1,-1],\n","     [-1,30,-1,-1,-1,-1,80,100],\n","     [-1,-1,-1,-1,30,-1,-1,100],\n","     [-1,-1,-1,-1,-1,-1,-1,100]]\n","\n","R = np.array(R)\n","print(\"reward matrix: matriz de recompensas\")\n","pd.DataFrame(R, \n","             index=['state_'+str(i) for i in range(R.shape[0])],\n","             columns=['state_'+str(i) for i in range(R.shape[0])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdxpGbNIW7yg"},"outputs":[],"source":["def melhor_caminho(Q, estado_inicial):\n","    #seu código aqui - crie uma lista L indicando qual o caminho sugerido pelo agente (representado pela matriz Q)\n","    return L"]},{"cell_type":"markdown","metadata":{"id":"ZerwRv1wW7yh"},"source":["<br><br><br><br><br>\n","Fim."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1ONBY77k6OBDbkiGYusFFLvMJyqhEs5LK","timestamp":1664305876610}]}},"nbformat":4,"nbformat_minor":0}